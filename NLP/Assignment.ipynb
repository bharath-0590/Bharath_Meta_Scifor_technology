{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Text Processing Methods **    ->  method of transforming  raw text data into a structured format that is usable for analysis, machine learning / natural language processing  tasks.    \n",
        "\n",
        "-> Involves a series of techniques to clean, manipulate text data and making it extracting meaningful insight's and patterns.\n",
        "\n",
        "\n",
        "**Methods**\n",
        "\n",
        "* Word Frequency\n",
        "\n",
        "* Collocation\n",
        "\n",
        "* TF-IDF\n",
        "\n",
        "* Text Summarization\n",
        "\n",
        "* Text classification\n",
        "\n",
        "* Keyword extraction\n",
        "\n",
        "* Lemmatization and stemming\n",
        "\n",
        "**Installations **\n"
      ],
      "metadata": {
        "id": "L8ONqm4TIJ0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "401iDj-DsED8",
        "outputId": "e13645ea-947c-455d-db7d-fb5de412ba71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXym8HM1Mp_y",
        "outputId": "3430e30d-db02-4e3f-a4f8-c6834acf186c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKTHIUsdM3Z1",
        "outputId": "6842a557-9e1a-4cc1-c479-a3e2d7a351ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**   ->\n",
        " splits text into individual words or sentences, allowing easier analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "dcQPzLwFUrzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,  sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "text = 'Hello, how are you?'\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "sentence = 'This is a simple sentence.'\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "#using Spacy\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('Hello, how are you?')\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbq9jyy4UqGl",
        "outputId": "47711119-6563-4268-d08e-872e9cf8c305"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'how', 'are', 'you', '?']\n",
            "['This', 'is', 'a', 'simple', 'sentence', '.']\n",
            "['Hello', ',', 'how', 'are', 'you', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Frequency**  -> counts occurances of each word & finding us common words in text."
      ],
      "metadata": {
        "id": "5uOFS-6utJEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize the text into words\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that enables machines to understand human language. Text processing is an essential part of NLP tasks.\"\n",
        "words = nltk.word_tokenize(text.lower())\n",
        "word_counts = Counter(words)\n",
        "\n",
        "print(\"Word Frequency:\", word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5oJy1KkN8TP",
        "outputId": "2fc6b3ac-e7d1-44cd-e59c-ccc056fba38f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency: Counter({'language': 2, 'processing': 2, 'nlp': 2, 'is': 2, '.': 2, 'natural': 1, '(': 1, ')': 1, 'a': 1, 'fascinating': 1, 'field': 1, 'that': 1, 'enables': 1, 'machines': 1, 'to': 1, 'understand': 1, 'human': 1, 'text': 1, 'an': 1, 'essential': 1, 'part': 1, 'of': 1, 'tasks': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Collocation**  ->\n",
        "define it's word pairs that appear together more frequently than expected by chance, like \"natural language\" in NLP."
      ],
      "metadata": {
        "id": "_QW4CfasOnNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "collocations = finder.nbest(bigram_measures.raw_freq, 10)\n",
        "print(\"Collocations:\", collocations)\n",
        "\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "collocations = finder.nbest(bigram_measures.raw_freq, 10)\n",
        "print(\"Collocations:\", collocations)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucuQ1jvrO3Xy",
        "outputId": "6a59aa2f-a407-437e-fb7b-11089dc8b0c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collocations: [('(', 'nlp'), (')', 'is'), ('.', 'text'), ('a', 'fascinating'), ('an', 'essential'), ('enables', 'machines'), ('essential', 'part'), ('fascinating', 'field'), ('field', 'that'), ('human', 'language')]\n",
            "Collocations: [('(', 'nlp'), (')', 'is'), ('.', 'text'), ('a', 'fascinating'), ('an', 'essential'), ('enables', 'machines'), ('essential', 'part'), ('fascinating', 'field'), ('field', 'that'), ('human', 'language')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concordance**  ->\n",
        "it displays each occurrence of a word in context, showing the surrounding text for quick analysis."
      ],
      "metadata": {
        "id": "Idza81g_PuSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the Gutenberg corpus\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "# Import the Gutenberg corpus\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Access the desired file from the Gutenberg corpus.\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Create a Text object\n",
        "text = nltk.Text(words)\n",
        "\n",
        "# Use the concordance method\n",
        "text.concordance('Happy', lines=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zBqTUR7P_e9",
        "outputId": "cbfc513d-e52e-4c5d-8b45-c4942bb9f454"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 10 of 123 matches:\n",
            "d rich , with a comfortable home and happy disposition , seemed to unite some o\n",
            " well in Brunswick Square . It was a happy circumstance , and animated Mr. Wood\n",
            "r about the wedding ; and I shall be happy to tell you , for we all behaved cha\n",
            " far as possible . And yet she was a happy woman , and a woman whom no one name\n",
            "ery frequently able to collect ; and happy was she , for her father 's sake , i\n",
            "icular pleasure in sending them away happy . The happiness of Miss Smith was qu\n",
            "a good deal ; she had spent two very happy months with them , and now loved to \n",
            "as can well be ; and while she is so happy at Hartfield , I can not wish her to\n",
            "Smith 's . '' Mr. Elton was only too happy . Harriet listened , and Emma drew i\n",
            ". `` By all means . We shall be most happy to consider you as one of the party \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Summarization** ->\n",
        " generates a shorter version of text that preserves important information. Here, we use the Gensim library’s summarization tool."
      ],
      "metadata": {
        "id": "Edx54MYmRg8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install summa package\n",
        "\n",
        "!pip install summa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8kpkwoEScy8",
        "outputId": "5dcb1a54-b1f6-4c42-9301-cd2af5eb8cc2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from summa) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.19->summa) (1.26.4)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54387 sha256=7d8d614e47f90c5a7a846b0e65b8d3bd547d66bb24a9b04050548a608b145898\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import summarizer\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that enables machines to understand human language. Text processing is an essential part of NLP tasks.\"\n",
        "\n",
        "summary = summarizer.summarize(text, ratio=0.9)\n",
        "print(\"Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr7d4SkvRw-u",
        "outputId": "ccbe4a09-3488-4b54-e428-b41efaecd763"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Natural Language Processing (NLP) is a fascinating field that enables machines to understand human language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Classification**   ->\n",
        " involves labeling text data with predefined categories. Here, we create a simple model using the Scikit-Learn library."
      ],
      "metadata": {
        "id": "lKGswkHuTX_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample data\n",
        "documents = [\"This is a positive document.\", \"This is a negative document.\", \"Another positive document.\"]\n",
        "labels = [\"positive\", \"negative\", \"positive\"]\n",
        "\n",
        "#convert text to feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "#train classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X, labels)\n",
        "\n",
        "#prediction\n",
        "new_documents = [\"This is a positive document.\", \"This is a neutral document.\"]\n",
        "new_X = vectorizer.transform(new_documents)\n",
        "predictions = classifier.predict(new_X)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hftv6tY8TiBa",
        "outputId": "4cef2695-4630-4f01-d86c-d155731376eb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: ['positive' 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop Words Removal**  ->\n",
        "Stop words are common words that often don't add significant meaning to text analysis."
      ],
      "metadata": {
        "id": "ntjONE5dtF4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = 'This is an example sentence with some stop words.'\n",
        "tokens = word_tokenize(text)\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(filtered_tokens)\n",
        "\n",
        "#using spacy\n",
        "doc = nlp(text)\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-SNK8eMtGYc",
        "outputId": "863af8ab-aba4-4851-b19f-6a6bfd4ad7f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'stop', 'words', '.']\n",
            "['example', 'sentence', 'stop', 'words', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Stemming**   ->\n",
        " reduces words to their base or root form by stripping suffixes."
      ],
      "metadata": {
        "id": "ekIqz_0Rtfs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "word = 'running'\n",
        "stemmed_word = stemmer.stem(word)\n",
        "print(stemmed_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGBLBFB4tgHz",
        "outputId": "ef3739a8-1b70-44c6-bd82-5339d913281f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy doesn’t provide stemming directly, but it has lemmatization  which is generally more accurate."
      ],
      "metadata": {
        "id": "PWvJkdkltssK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization** ->\n",
        " converts words to their dictionary form or lemma."
      ],
      "metadata": {
        "id": "q2ztlzaiVui0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = 'running'\n",
        "lemmatized_word = lemmatizer.lemmatize(word)\n",
        "print(lemmatized_word)\n",
        "\n",
        "#using spacy\n",
        "doc = nlp(word)\n",
        "lemmatized_word = doc[0].lemma_\n",
        "print(lemmatized_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSYkAxY1ttRq",
        "outputId": "509739ad-1727-4684-a010-82de6b2932fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Normalization**  ->  includes tasks like lowercasing, removing punctuation, and expanding contractions."
      ],
      "metadata": {
        "id": "7DlpD0yIurx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text normalization\n",
        "\n",
        "import re\n",
        "\n",
        "#lowercase text\n",
        "text = 'Hello, How are you?'\n",
        "lowercase_text = text.lower()\n",
        "print(lowercase_text)\n",
        "\n",
        "#remove punctuation\n",
        "text = 'Hello, How are you?'\n",
        "punctuation_removed_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(punctuation_removed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4n1YEZnusPa",
        "outputId": "f2a6dda6-9bea-463d-f9a3-9bdce74aabd5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello, how are you?\n",
            "Hello How are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings** / Vectorization  ->\n",
        " convert words into numerical vectors representing semantic meaning."
      ],
      "metadata": {
        "id": "SzJqHhDjvAdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using TF-IDF with skikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.']\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "#word embeddings (spacy)\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('This is an example sentence.')\n",
        "word_embeddings = [token.vector for token in doc]\n",
        "print(word_embeddings)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy4ptVVbvAzB",
        "outputId": "fec9ce79-9abe-4292-ca05-4a43f217a17c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
            "  0.3645444  0.         0.3645444 ]\n",
            " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
            "  0.28285122 0.         0.28285122]\n",
            " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
            "  0.29360705 0.49711994 0.29360705]]\n",
            "[array([-0.33933002, -0.49662387,  0.12057114,  1.6358336 ,  0.60533595,\n",
            "        0.29953307,  1.3407537 ,  0.19745567, -1.3807919 , -0.1795856 ,\n",
            "        2.000516  ,  1.3099126 , -0.84563535, -0.30392313, -1.1996465 ,\n",
            "       -1.0611061 , -0.11863522,  1.4365962 , -0.98706937,  1.056712  ,\n",
            "       -1.088603  ,  0.00695902, -0.20875025, -1.1293292 , -0.3502592 ,\n",
            "        0.09407187,  0.2085126 ,  0.9926027 , -0.25046033,  0.257219  ,\n",
            "        0.08152232, -0.8692955 , -0.12662315, -1.1390253 , -0.4092511 ,\n",
            "        0.725322  ,  0.14983839, -0.38918096,  0.2666754 ,  3.5760899 ,\n",
            "       -0.7669859 , -0.06527622, -0.733409  ,  2.247148  , -1.3842859 ,\n",
            "        1.1196162 , -0.4907301 ,  1.2082839 ,  1.1484601 , -0.9420607 ,\n",
            "       -1.2381608 ,  0.16508707, -0.13466397, -0.02907619,  0.56175435,\n",
            "       -1.436982  , -0.71504205, -0.48806912, -0.71487296,  0.4807003 ,\n",
            "        1.4431789 , -0.02584107, -0.64125884, -0.08943826,  1.4952669 ,\n",
            "       -0.14781511,  0.60827625, -1.4152248 , -0.76082444, -0.9266565 ,\n",
            "       -0.02084343, -1.0318109 , -0.4344411 , -0.27344283, -0.36764538,\n",
            "       -0.06356841,  0.9969431 , -0.5117375 ,  0.92545474, -0.07288869,\n",
            "       -0.0635286 , -0.84988576,  0.13359603, -1.3517065 ,  1.3519024 ,\n",
            "        1.7034628 ,  1.1009651 ,  0.12837519,  0.77116185, -0.41909897,\n",
            "       -0.49402988, -0.55327713,  0.5444661 , -0.6831821 , -0.9644559 ,\n",
            "        0.11981507], dtype=float32), array([-0.2966698 ,  1.2215282 , -0.7286844 ,  0.45900345,  0.15150368,\n",
            "       -0.86329234, -0.32365248, -0.13644844,  0.97238255,  0.62473696,\n",
            "        0.97500914,  0.07251963, -0.49626333,  1.2149802 , -0.44948328,\n",
            "        0.50609595, -1.194672  , -0.12908602, -0.26848817,  0.8792242 ,\n",
            "       -0.40355575, -0.68502444,  0.35319376,  1.0469885 , -0.8334773 ,\n",
            "       -0.3771012 ,  0.23595122,  0.07568955, -0.37941673, -0.72747946,\n",
            "       -0.32091108,  2.0630147 ,  0.8731485 ,  0.01422273,  0.16291276,\n",
            "       -0.8078397 ,  0.32167044, -0.9747248 ,  0.6633295 , -1.436525  ,\n",
            "        1.5937454 ,  0.354954  ,  1.0574412 , -1.2278322 ,  1.1671958 ,\n",
            "        0.5816356 ,  1.706618  , -0.18421401,  0.14172694, -0.6091523 ,\n",
            "       -0.8896967 ,  0.09674472, -1.1230664 , -0.5800711 , -0.06729992,\n",
            "       -0.7769204 ,  0.16147138, -0.35487902, -0.6240338 ,  0.54101276,\n",
            "        0.76646346, -0.1429488 ,  0.7568739 ,  0.6475711 ,  1.1877211 ,\n",
            "        1.0945686 ,  0.10314629, -0.69355166,  1.099384  ,  0.32485428,\n",
            "       -1.0188303 ,  0.2691065 , -0.68035555, -0.10310151, -0.8195565 ,\n",
            "        0.11477264,  0.2171334 , -1.0312638 , -1.239593  ,  0.4796397 ,\n",
            "       -0.34471798, -0.29233855, -0.14245653, -0.87958235,  1.1130068 ,\n",
            "       -0.3156153 ,  0.16491719,  0.18290105, -0.5873804 , -0.6199825 ,\n",
            "       -0.05526534, -1.0994011 , -0.55542606, -0.3758652 , -1.0359975 ,\n",
            "       -0.11300439], dtype=float32), array([ 2.0576122 ,  0.44752327,  0.9976011 ,  0.8472536 ,  0.0553996 ,\n",
            "       -0.47225326,  0.3352023 , -0.12944773, -0.5548062 ,  2.15836   ,\n",
            "       -0.39304447,  0.43340042,  0.8389678 ,  0.80357885, -1.1442629 ,\n",
            "       -0.40934938, -0.43007344, -0.33091754, -0.49614844,  0.5632918 ,\n",
            "       -0.24262848, -0.79459316,  1.3271561 , -1.6351712 ,  1.0949395 ,\n",
            "       -1.5116912 ,  0.7899516 , -0.9377526 ,  0.83711666,  2.0959542 ,\n",
            "        1.2339127 ,  0.60640764,  0.42845836, -1.198417  , -0.25125104,\n",
            "        0.66830957,  0.8343131 ,  1.1274514 , -0.39262134, -0.9084189 ,\n",
            "       -0.32125276,  0.8809283 , -0.425606  ,  0.30663306, -1.0897185 ,\n",
            "        1.6958705 , -2.0163417 ,  0.5229834 , -0.02384655,  0.23566285,\n",
            "       -0.9325789 , -0.1554116 ,  0.30987895,  1.5787772 ,  0.04855603,\n",
            "        0.7850886 , -1.7932527 ,  1.9366543 , -0.09809136, -0.21176453,\n",
            "        0.46124214, -1.4302808 ,  0.141146  , -1.553268  ,  0.01785792,\n",
            "       -0.7628615 ,  0.5223042 , -0.10187855,  0.01069754,  0.6442077 ,\n",
            "       -0.43730736, -0.1691212 , -1.1730831 ,  1.2725277 , -0.77450323,\n",
            "       -1.2616858 , -0.04799631,  0.5702973 , -0.29024693, -0.06319031,\n",
            "        0.35887754, -0.44967037,  0.3056111 , -1.011275  ,  0.14869966,\n",
            "       -0.52480143,  1.3539025 , -0.35173965, -0.7381084 ,  0.85438037,\n",
            "        0.29975027, -0.6296979 ,  0.07666911, -1.2643306 , -0.48513177,\n",
            "       -1.1515547 ], dtype=float32), array([ 1.2448637 , -0.88775945,  1.2269104 , -0.353697  , -0.05662211,\n",
            "       -0.21700534,  0.10272804, -0.27767965,  0.382592  ,  0.01345104,\n",
            "       -0.65053916,  0.03174669, -0.14765057,  0.47893673,  0.08128345,\n",
            "       -0.06366195, -0.20878576, -0.9773966 , -0.16550301, -0.56870234,\n",
            "        0.6109846 ,  0.32001096,  0.36027592,  0.31800294, -0.30307382,\n",
            "       -0.71520483,  0.49801606,  0.32085067,  2.1184688 ,  0.8706917 ,\n",
            "        0.5102105 ,  0.8392057 , -0.93092775, -1.0926466 ,  0.23423654,\n",
            "       -0.40757117,  0.18369748, -0.5699688 , -0.04451525,  0.34386992,\n",
            "       -0.5422121 ,  1.8392221 ,  0.24909636,  1.3936963 ,  0.5472797 ,\n",
            "       -0.04855368, -1.3627545 , -0.5544437 , -0.677659  , -0.62779474,\n",
            "       -0.5445934 ,  0.79974794,  1.4064784 , -1.4717203 , -0.13524139,\n",
            "       -0.3662315 , -0.00693834, -0.09312765, -0.82266223, -0.03339484,\n",
            "       -0.3679716 , -0.84457   , -0.12060622, -1.1644826 ,  0.6667952 ,\n",
            "       -1.1562662 ,  0.6681323 ,  0.1837801 ,  0.40612787, -0.5369527 ,\n",
            "        0.99941695,  1.5370001 , -0.62035036,  0.89756596, -0.11213326,\n",
            "       -0.5451015 , -1.253309  , -1.4484321 , -0.58810556,  0.5918313 ,\n",
            "       -0.03233933, -1.3619407 ,  0.69495976,  0.08309313, -1.2702638 ,\n",
            "       -0.06653753,  0.93915445, -0.12250769, -0.91438663,  0.51230735,\n",
            "        0.36539653, -0.6110886 ,  1.6050308 ,  0.8756941 , -1.2191945 ,\n",
            "        0.3715198 ], dtype=float32), array([-0.8655341 ,  0.11102453, -0.3861472 , -0.20034549, -0.22509325,\n",
            "       -0.22004989,  0.90778226,  1.2310746 ,  0.13040224, -1.2780743 ,\n",
            "       -1.0507077 , -0.7847379 , -0.42390287,  0.29053628, -0.13307118,\n",
            "        0.13532314,  0.20219952, -1.2687733 ,  1.0204433 ,  0.77249616,\n",
            "        0.62223536,  0.604481  , -0.08232825, -0.69891393,  0.01384234,\n",
            "       -0.71267295, -0.38414192, -0.38532865,  0.5656201 ,  0.46470454,\n",
            "       -0.30397823, -0.20529497,  0.23283333, -0.1949132 , -0.09206289,\n",
            "        0.05819586, -0.2750845 ,  0.7719739 , -0.01010753, -0.38333535,\n",
            "       -0.47824252,  0.6252294 ,  0.25406536,  0.8613224 , -0.40583163,\n",
            "        0.50021243, -1.4967153 , -0.8424523 , -0.7395272 , -0.31787956,\n",
            "        0.3184207 ,  1.4618826 ,  0.9126967 , -0.9339975 ,  1.179905  ,\n",
            "       -0.177419  , -0.4432482 ,  0.41187394,  0.2538331 , -1.1503133 ,\n",
            "       -0.54738605, -0.02858841,  0.38909966,  0.06832942,  0.99724865,\n",
            "        1.2687714 , -0.25935358, -1.4331218 , -0.35996145, -0.4389339 ,\n",
            "        0.21025044, -0.5543033 ,  1.5312734 , -0.21993044, -0.6825268 ,\n",
            "       -0.69571453, -0.82093406, -0.7478126 ,  0.03169496, -1.2316184 ,\n",
            "       -0.2585333 , -0.19612086, -1.0246973 , -0.22909272,  0.51265687,\n",
            "        1.4359709 ,  0.6460053 , -0.54175574, -0.45986298,  0.8662883 ,\n",
            "       -0.23923063, -0.05764796,  1.4261503 ,  0.8716794 , -0.4948059 ,\n",
            "        1.6063416 ], dtype=float32), array([-0.6112993 , -0.8411312 ,  0.00931601, -1.6200247 , -0.2382927 ,\n",
            "        2.244028  ,  0.64976287, -0.1221094 , -0.12855823, -0.82051826,\n",
            "        3.3200557 , -0.86812955,  0.63061476,  2.4956632 , -0.03469162,\n",
            "        0.18521246, -1.6678691 , -0.86800337,  1.363537  , -1.0583037 ,\n",
            "        0.6820776 , -0.55494106,  1.9783533 , -0.31898665,  1.6920509 ,\n",
            "        3.6417768 , -0.6436596 ,  0.22753501, -0.5238236 ,  1.8973553 ,\n",
            "        0.02739549,  1.3581048 ,  1.2152387 , -0.09284822, -0.38577607,\n",
            "        0.18751943, -0.20709655, -0.67731786, -1.0069859 ,  1.1362977 ,\n",
            "        2.249475  , -0.08805209,  0.11065357, -0.87075186,  0.10123733,\n",
            "       -0.9692577 , -0.7896918 , -0.1731401 ,  1.5114197 , -0.5628907 ,\n",
            "        3.5097883 , -0.8389041 , -0.6799269 ,  0.22930951,  0.78373337,\n",
            "       -1.2818699 , -0.61673564, -0.96597683,  0.65658   , -0.6004645 ,\n",
            "        0.05306102, -1.6472943 ,  0.3659535 ,  2.2230027 ,  0.32054338,\n",
            "       -0.21673831, -0.70770764,  0.19298515, -0.79946   , -0.45457777,\n",
            "       -0.6393423 , -0.03772559, -1.0798601 , -0.40616244,  2.6686516 ,\n",
            "        0.03053528, -0.68173987,  1.2897882 , -1.0170934 , -1.5122766 ,\n",
            "       -0.5057372 , -1.6974401 , -0.5693751 ,  0.31825882, -1.1437436 ,\n",
            "       -0.09709707, -0.49205726,  1.1490722 , -1.3863597 ,  0.9737873 ,\n",
            "       -0.76938075,  0.03890961,  0.04879472, -0.16544491, -0.76240015,\n",
            "       -0.83743787], dtype=float32)]\n"
          ]
        }
      ]
    }
  ]
}